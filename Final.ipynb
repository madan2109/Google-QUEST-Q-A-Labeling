{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle\n",
    "import re\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tokenization\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(data):\n",
    "    data = pd.read_csv(\"data.csv\")   #reading data from csv file\n",
    "    \n",
    "    #features required processing data\n",
    "    text_columns = [\"question_title\",\"question_body\",\"answer\"]  \n",
    "\n",
    "    #This below code for removing html tags  \n",
    "    TAG_RE = re.compile(r'<[^>]+>')\n",
    "    def remove_tags(text):\n",
    "        return TAG_RE.sub('', text)\n",
    "\n",
    "    for i in data.index:\n",
    "        for j in text_columns:\n",
    "            data.at[i,j] = remove_tags(data.loc[i,j])\n",
    "    # dictionary of data for elaborating the words \n",
    "    eloborate_dict = {\"won't\":\"will not\",\n",
    "                        \"can\\'t\":\"can not\",\n",
    "                        \"n\\'t\":\" not\",\n",
    "                        \"\\'re\":\" are\",\n",
    "                        \"\\'s\":\" is\",\n",
    "                        \"\\'d\":\" would\",\n",
    "                        \"\\'ll\":\" will\",\n",
    "                        \"\\'t\":\" not\",\n",
    "                        \"\\'ve\":\" have\",\n",
    "                        \"\\'m\":\" am\"\n",
    "                    }\n",
    "    \n",
    "    #function for elaborating the text\n",
    "    def replace_(text):\n",
    "        for i,j in eloborate_dict.items(): \n",
    "            text = re.sub(i,j, text)\n",
    "        return text\n",
    "\n",
    "    #function for removing the special symbols\n",
    "    def replace_special(text): \n",
    "        text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+=]\", \" \", text)\n",
    "        text = re.sub(r\"\\-\", \" \", text)\n",
    "        text = re.sub(r\"\\/\", \"\", text)\n",
    "        text = re.sub(r\"\\.\", \"\", text)\n",
    "        text = re.sub(r\"\\,\", \"\", text)\n",
    "        text = re.sub(r\"\\n\", \"\", text)\n",
    "        text = re.sub(r\"\\'\", \"\", text)\n",
    "        text = re.sub(r\"\\=\", \"\", text)\n",
    "        text = re.sub(r\"\\+\", \"\", text)\n",
    "        text = re.sub(r\"\\^\", \"\", text)\n",
    "    \n",
    "        return text\n",
    "    \n",
    "    \n",
    "    for col in text_columns:\n",
    "        data[col] = data[col].apply(lambda x: replace_(x.lower()))\n",
    "    \n",
    "    for col in text_columns:\n",
    "        data[col] = data[col].apply(lambda x: replace_special(x))\n",
    "        \n",
    "        \n",
    "    with open('vocab_file.pkl','rb') as f:\n",
    "        vocab_file = pickle.load(f)\n",
    "\n",
    "    with open('do_lower_case.pkl','rb') as f:\n",
    "        do_lower_case = pickle.load(f)\n",
    "    \n",
    "    #tokenizing the sentences\n",
    "    tokenizer = tokenization.FullTokenizer(vocab_file,do_lower_case)\n",
    "    \n",
    "    #Processing Question title feature\n",
    "    data_qt_tokens = np.zeros(shape=(data.shape[0],25))\n",
    "    data_qt_mask = np.zeros(shape=(data.shape[0],25))\n",
    "    data_qt_segment = np.zeros(shape=(data.shape[0],25))\n",
    "    max_seq_length = 25\n",
    "    for i in range(data.shape[0]):\n",
    "        tokens = tokenizer.tokenize(data.values[i][0])\n",
    "        if len(tokens) >= max_seq_length-2:\n",
    "            tokens = tokens[0:(max_seq_length-2)]\n",
    "            tokens = ['[CLS]',*tokens,'[SEP]']\n",
    "            data_qt_tokens[i] = np.array(tokenizer.convert_tokens_to_ids(tokens))\n",
    "            data_qt_mask[i] = np.array([1]*len(tokens)+[0]*(max_seq_length-len(tokens)))\n",
    "            data_qt_segment[i] = np.array([0]*max_seq_length)\n",
    "        else:\n",
    "            tokens = ['[CLS]',*tokens,'[SEP]']\n",
    "            data_qt_mask[i] = np.array([1]*len(tokens)+[0]*(max_seq_length-len(tokens)))\n",
    "            tokens = tokens + ['[PAD]']*(max_seq_length-len(tokens))\n",
    "            data_qt_tokens[i] = np.array(tokenizer.convert_tokens_to_ids(tokens))\n",
    "            data_qt_segment[i] = np.array([0]*max_seq_length)\n",
    "            \n",
    "            \n",
    "    bert_model_1 = tf.keras.models.load_model('bert_model_1.h5',custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "\n",
    "    data_qt_pooled_output = bert_model_1.predict([data_qt_tokens.astype('int32'), data_qt_mask.astype('int32'), \n",
    "                                                   data_qt_segment.astype('int32')])\n",
    "    \n",
    "    #Processing Question body feature\n",
    "    data_q_tokens = np.zeros(shape=(data.shape[0],512))\n",
    "    data_q_mask = np.zeros(shape=(data.shape[0],512))\n",
    "    data_q_segment = np.zeros(shape=(data.shape[0],512))\n",
    "    max_seq_length = 512\n",
    "    for i in range(data.shape[0]):\n",
    "        tokens = tokenizer.tokenize(data.values[i][1])\n",
    "        if len(tokens) >= max_seq_length-2:\n",
    "            tokens = tokens[0:(max_seq_length-2)]\n",
    "            tokens = ['[CLS]',*tokens,'[SEP]']\n",
    "            data_q_tokens[i] = np.array(tokenizer.convert_tokens_to_ids(tokens))\n",
    "            data_q_mask[i] = np.array([1]*len(tokens)+[0]*(max_seq_length-len(tokens)))\n",
    "            data_q_segment[i] = np.array([0]*max_seq_length)\n",
    "        else:\n",
    "            tokens = ['[CLS]',*tokens,'[SEP]']\n",
    "            data_q_mask[i] = np.array([1]*len(tokens)+[0]*(max_seq_length-len(tokens)))\n",
    "            tokens = tokens + ['[PAD]']*(max_seq_length-len(tokens))\n",
    "            data_q_tokens[i] = np.array(tokenizer.convert_tokens_to_ids(tokens))\n",
    "            data_q_segment[i] = np.array([0]*max_seq_length)\n",
    "            \n",
    "    bert_model_2 = tf.keras.models.load_model('bert_model_2.h5',custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "\n",
    "    data_q_pooled_output = bert_model_2.predict([data_q_tokens.astype('int32'), data_q_mask.astype('int32'), \n",
    "                                                   data_q_segment.astype('int32')])\n",
    "\n",
    "    #Processing Answer feature\n",
    "    data_a_tokens = np.zeros(shape=(data.shape[0],512))\n",
    "    data_a_mask = np.zeros(shape=(data.shape[0],512))\n",
    "    data_a_segment = np.zeros(shape=(data.shape[0],512))\n",
    "    max_seq_length = 512\n",
    "    for i in range(data.shape[0]):\n",
    "        tokens = tokenizer.tokenize(data.values[i][2])\n",
    "        if len(tokens) >= max_seq_length-2:\n",
    "            tokens = tokens[0:(max_seq_length-2)]\n",
    "            tokens = ['[CLS]',*tokens,'[SEP]']\n",
    "            data_a_tokens[i] = np.array(tokenizer.convert_tokens_to_ids(tokens))\n",
    "            data_a_mask[i] = np.array([1]*len(tokens)+[0]*(max_seq_length-len(tokens)))\n",
    "            data_a_segment[i] = np.array([0]*max_seq_length)\n",
    "        else:\n",
    "            tokens = ['[CLS]',*tokens,'[SEP]']\n",
    "            data_a_mask[i] = np.array([1]*len(tokens)+[0]*(max_seq_length-len(tokens)))\n",
    "            tokens = tokens + ['[PAD]']*(max_seq_length-len(tokens))\n",
    "            data_a_tokens[i] = np.array(tokenizer.convert_tokens_to_ids(tokens))\n",
    "            data_a_segment[i] = np.array([0]*max_seq_length)\n",
    "            \n",
    "    data_a_pooled_output = bert_model_2.predict([data_a_tokens.astype('int32'), data_a_mask.astype('int32'), \n",
    "                                                   data_a_segment.astype('int32')])\n",
    "\n",
    "    \n",
    "    data_comp = [data_qt_pooled_output,data_q_pooled_output,data_a_pooled_output]\n",
    "\n",
    "    bert_model = tf.keras.models.load_model('model_bert.h5')\n",
    "    \n",
    "    #predicting the 30 features\n",
    "    y_pred = model_bert.predict(data_comp)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(data,output):\n",
    "    data = pd.read_csv(\"data.csv\")   #reading data from csv file\n",
    "    \n",
    "    #features required processing data\n",
    "    text_columns = [\"question_title\",\"question_body\",\"answer\"]  \n",
    "\n",
    "    #This below code for removing html tags  \n",
    "    TAG_RE = re.compile(r'<[^>]+>')\n",
    "    def remove_tags(text):\n",
    "        return TAG_RE.sub('', text)\n",
    "\n",
    "    for i in data.index:\n",
    "        for j in text_columns:\n",
    "            data.at[i,j] = remove_tags(data.loc[i,j])\n",
    "    # dictionary of data for elaborating the words \n",
    "    eloborate_dict = {\"won't\":\"will not\",\n",
    "                        \"can\\'t\":\"can not\",\n",
    "                        \"n\\'t\":\" not\",\n",
    "                        \"\\'re\":\" are\",\n",
    "                        \"\\'s\":\" is\",\n",
    "                        \"\\'d\":\" would\",\n",
    "                        \"\\'ll\":\" will\",\n",
    "                        \"\\'t\":\" not\",\n",
    "                        \"\\'ve\":\" have\",\n",
    "                        \"\\'m\":\" am\"\n",
    "                    }\n",
    "    \n",
    "    #function for elaborating the text\n",
    "    def replace_(text):\n",
    "        for i,j in eloborate_dict.items(): \n",
    "            text = re.sub(i,j, text)\n",
    "        return text\n",
    "\n",
    "    #function for removing the special symbols\n",
    "    def replace_special(text): \n",
    "        text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+=]\", \" \", text)\n",
    "        text = re.sub(r\"\\-\", \" \", text)\n",
    "        text = re.sub(r\"\\/\", \"\", text)\n",
    "        text = re.sub(r\"\\.\", \"\", text)\n",
    "        text = re.sub(r\"\\,\", \"\", text)\n",
    "        text = re.sub(r\"\\n\", \"\", text)\n",
    "        text = re.sub(r\"\\'\", \"\", text)\n",
    "        text = re.sub(r\"\\=\", \"\", text)\n",
    "        text = re.sub(r\"\\+\", \"\", text)\n",
    "        text = re.sub(r\"\\^\", \"\", text)\n",
    "    \n",
    "        return text\n",
    "    \n",
    "    \n",
    "    for col in text_columns:\n",
    "        data[col] = data[col].apply(lambda x: replace_(x.lower()))\n",
    "    \n",
    "    for col in text_columns:\n",
    "        data[col] = data[col].apply(lambda x: replace_special(x))\n",
    "        \n",
    "        \n",
    "    with open('vocab_file.pkl','rb') as f:\n",
    "        vocab_file = pickle.load(f)\n",
    "\n",
    "    with open('do_lower_case.pkl','rb') as f:\n",
    "        do_lower_case = pickle.load(f)\n",
    "    \n",
    "    #tokenizing the sentences\n",
    "    tokenizer = tokenization.FullTokenizer(vocab_file,do_lower_case)\n",
    "    \n",
    "    #Processing Question title feature\n",
    "    data_qt_tokens = np.zeros(shape=(data.shape[0],25))\n",
    "    data_qt_mask = np.zeros(shape=(data.shape[0],25))\n",
    "    data_qt_segment = np.zeros(shape=(data.shape[0],25))\n",
    "    max_seq_length = 25\n",
    "    for i in range(data.shape[0]):\n",
    "        tokens = tokenizer.tokenize(data.values[i][0])\n",
    "        if len(tokens) >= max_seq_length-2:\n",
    "            tokens = tokens[0:(max_seq_length-2)]\n",
    "            tokens = ['[CLS]',*tokens,'[SEP]']\n",
    "            data_qt_tokens[i] = np.array(tokenizer.convert_tokens_to_ids(tokens))\n",
    "            data_qt_mask[i] = np.array([1]*len(tokens)+[0]*(max_seq_length-len(tokens)))\n",
    "            data_qt_segment[i] = np.array([0]*max_seq_length)\n",
    "        else:\n",
    "            tokens = ['[CLS]',*tokens,'[SEP]']\n",
    "            data_qt_mask[i] = np.array([1]*len(tokens)+[0]*(max_seq_length-len(tokens)))\n",
    "            tokens = tokens + ['[PAD]']*(max_seq_length-len(tokens))\n",
    "            data_qt_tokens[i] = np.array(tokenizer.convert_tokens_to_ids(tokens))\n",
    "            data_qt_segment[i] = np.array([0]*max_seq_length)\n",
    "            \n",
    "            \n",
    "    bert_model_1 = tf.keras.models.load_model('bert_model_1.h5',custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "\n",
    "    data_qt_pooled_output = bert_model_1.predict([data_qt_tokens.astype('int32'), data_qt_mask.astype('int32'), \n",
    "                                                   data_qt_segment.astype('int32')])\n",
    "    \n",
    "    #Processing Question body feature\n",
    "    data_q_tokens = np.zeros(shape=(data.shape[0],512))\n",
    "    data_q_mask = np.zeros(shape=(data.shape[0],512))\n",
    "    data_q_segment = np.zeros(shape=(data.shape[0],512))\n",
    "    max_seq_length = 512\n",
    "    for i in range(data.shape[0]):\n",
    "        tokens = tokenizer.tokenize(data.values[i][1])\n",
    "        if len(tokens) >= max_seq_length-2:\n",
    "            tokens = tokens[0:(max_seq_length-2)]\n",
    "            tokens = ['[CLS]',*tokens,'[SEP]']\n",
    "            data_q_tokens[i] = np.array(tokenizer.convert_tokens_to_ids(tokens))\n",
    "            data_q_mask[i] = np.array([1]*len(tokens)+[0]*(max_seq_length-len(tokens)))\n",
    "            data_q_segment[i] = np.array([0]*max_seq_length)\n",
    "        else:\n",
    "            tokens = ['[CLS]',*tokens,'[SEP]']\n",
    "            data_q_mask[i] = np.array([1]*len(tokens)+[0]*(max_seq_length-len(tokens)))\n",
    "            tokens = tokens + ['[PAD]']*(max_seq_length-len(tokens))\n",
    "            data_q_tokens[i] = np.array(tokenizer.convert_tokens_to_ids(tokens))\n",
    "            data_q_segment[i] = np.array([0]*max_seq_length)\n",
    "            \n",
    "    bert_model_2 = tf.keras.models.load_model('bert_model_2.h5',custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "\n",
    "    data_q_pooled_output = bert_model_2.predict([data_q_tokens.astype('int32'), data_q_mask.astype('int32'), \n",
    "                                                   data_q_segment.astype('int32')])\n",
    "\n",
    "    #Processing Answer feature\n",
    "    data_a_tokens = np.zeros(shape=(data.shape[0],512))\n",
    "    data_a_mask = np.zeros(shape=(data.shape[0],512))\n",
    "    data_a_segment = np.zeros(shape=(data.shape[0],512))\n",
    "    max_seq_length = 512\n",
    "    for i in range(data.shape[0]):\n",
    "        tokens = tokenizer.tokenize(data.values[i][2])\n",
    "        if len(tokens) >= max_seq_length-2:\n",
    "            tokens = tokens[0:(max_seq_length-2)]\n",
    "            tokens = ['[CLS]',*tokens,'[SEP]']\n",
    "            data_a_tokens[i] = np.array(tokenizer.convert_tokens_to_ids(tokens))\n",
    "            data_a_mask[i] = np.array([1]*len(tokens)+[0]*(max_seq_length-len(tokens)))\n",
    "            data_a_segment[i] = np.array([0]*max_seq_length)\n",
    "        else:\n",
    "            tokens = ['[CLS]',*tokens,'[SEP]']\n",
    "            data_a_mask[i] = np.array([1]*len(tokens)+[0]*(max_seq_length-len(tokens)))\n",
    "            tokens = tokens + ['[PAD]']*(max_seq_length-len(tokens))\n",
    "            data_a_tokens[i] = np.array(tokenizer.convert_tokens_to_ids(tokens))\n",
    "            data_a_segment[i] = np.array([0]*max_seq_length)\n",
    "            \n",
    "    data_a_pooled_output = bert_model_2.predict([data_a_tokens.astype('int32'), data_a_mask.astype('int32'), \n",
    "                                                   data_a_segment.astype('int32')])\n",
    "\n",
    "    \n",
    "    data_comp = [data_qt_pooled_output,data_q_pooled_output,data_a_pooled_output]\n",
    "\n",
    "    bert_model = tf.keras.models.load_model('model_bert.h5')\n",
    "    \n",
    "    #predicting the 30 features\n",
    "    y_pred = model_bert.predict(data_comp)\n",
    "    \n",
    "    metric_spearman = np.mean([spearmanr(output[:, ind], y_pred[:, ind]).correlation for ind in range(y_pred_val.shape[1])])\n",
    "    \n",
    "    return metric_spearman"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
